### What is AI ?
The term is often used to describe the process of machine being able to autonomusly take decision and perform tasks close to the human intelligence level
### Laungage AI
Langague AI is the subfield of AI which focuses on technologies focused on understanding, processing and generating human laguages.
The term can be intercahanglably used with NLP but the scope expands further where the LLM technologies encompasses things that might not directly be langugage models but that can have significant impact on the field <br>

### History of Langague Models
1. Bag of Words
2. Word2Vec
3. Attention
4. BERT
5. GPT
5. GPT 2 
6. Roberta
and many more including ChatGPT and Gemini

### Model Elobrated Understanding <br>
1. Bag of Words: Contains below steps
   A. Tokenization: Breaks the statements in tokens (E.g. "Cat sat on a mat" corresponding tokens will be ['Cat','sat','on','a','mat']
   B. Create a dictionary from the individual tokens based on the entire document
   C. Represent the original sentence based on the numerical representation depending presence of the word in sentence (1 if the word exists, 0 if not)

**Limitation**
A. Final representation is just a bag of word without any sementaic meaning E.g. "Tiger ate rabbit" and "Rabbit ate tiger" would essentially be the same sentences

2. **Word2Vec**: It is a NN based model that represents tokens in the high multidimensional space in the form of vector. Words having similar meaning or that appear together in similar context usually are closer to each other compared to the ones 
   that have very different meaning or don't appear toghther often
   The way model works is as below
   1. Words are intially embedded and represented in the form of vectors, and the initial values are randomized
   2. Eventually as the training data is passed to the model, the model tries to optimize similar words (with meaning/context) closer to each other
   3. End output is the vector representation of the words in the high dimensional space

**Advantages**
Preserves some degree of semantic meaning

**Limitation**
The end output is a static representation of the words. However, if the same word appears in a very different context tha was not present in the training data the results are not tha accurate <br>
E.g. "I am standing at a bank". Here the word "bank" can have various meanings depending on the context which can't be adjusted for in the static represenation

3. "**Encoder Decoder with Attention**
This is a RNN based archtiecture on both Encoder as well as decoder side. <br>
Encoder Side
Let's take an example of Machine Translation where we want to translate english sentence "I love India" to Hindi.

Step 1: 
**Tokenization**
Input sentence will be broken down into individual tokens ['I','Love','India'] as </br>
x1 -> 'I' </br>
x2 -> 'love' </br>
x3 -> 'India' </br>

The above generated tokens would then be passed to the encoder sequentially.</br>
Encoder architecture underneath is a RNN based NN which would process these tokens sequentially.<br>

At time t=1 <br>
Encoder would have two values </br>
A. Input token : "I" </br>
B. Prev. hidden state = h0 (which would be essentially 0 since this is first step in the process <br>

Encder then process h0 and generates h1
h1 = F(h0,x1) </br>

At time t = 2 </br>
X2 = "love"</br>
pev. hidden state = h1 </br>
h2 = F(h1, x2)</br>
the process repeats till all the tokens are processeed and the final hidden state is generated h4 in this example for token x4 ='country'<br>

Once all the tokens are processed by the encoder, then the decoder comes into picture. </br>
Decoder also is a RNN based architecture. <br>
Please note that decoder starts to function only after the entire tokens are encoded. And the decoder timeline mentioned below is indepndent of the encoder timeline<br>
At time t=0 <br>
Decoder hidden state represented by s0 will be h4 i.e. last output generated by encoder<br>
Btw, if you are wondering what does encoder hidden state means.<br> 
Essentially the encoder hiddens state is the memory representation of the generated output <br>
 A. It contains important information such as the vector representaion of generated token as well as <br>
 B. Information related to positional sequence of the tokens generated so far <br>
 
Now coming back at time t=0 <br>
we have s0 = h4, y0 = <SOS> [for those wondering, <SOS> is a special token that is added at the start of any sentence by decoder itself and hence our y0 is holds value even we haven't generated any token so far] </br>
and finally the attention mechanism represented by context vector c0 = h4 (here we have randomly initialized it to be final state of encoder since we haven't generated any attention scores yet) </br>

 </br>
 Next hidden state s1 = DecoderRNN(s0, h0, c0)

**Attention Mechanism:** The attention mechanism is best described by its utility, this mechanism ensures that depending on the output <br>
that is being decoded the relevant parts in the sentence gets more weights compared to the one that are less relevant <br>.
To give it a perspective: E.g. "Animal couldn't cross the road because **it** was injurd".<br>
In the above sentence what does the word **it** refers to ? Does **it** refers to animal or road ? <br>
While for us humans it might be very intuvitue to differntiate and answer the above question, howsoever it isn't that easy for machines </br>
Hence in order to avoid any such confusion attention vector comes in very handy.<br>


Now instead of just using the final state from the encoder which is present as s0 = h3 <br>
Context vector c0 will be a linear combination of h1, h2, h3 and h4 <br>
Hence c1 = ⍺11*h1 + ⍺12*h2 + ⍺13*h3 + ⍺14*h4 <br>
**⍺**in the above equation is called as attention weight <br>
How it is calculated </br>
⍺1i which repesents the attention score for decoder hidden state 1 w.r.t i'th encoder hidden state <br>
First a similarity score between current hidden state s1 and every encoder hidden state is computed using a vector dot product <br>
score = score(s1, hi) <1> </br>
And then this is passed thorough a softmax function to obtain the attention score, which is a normalization process and ensures sum of indiviudal weights <br>
generted for specific context vector c1 i.e. ⍺11, ⍺12, ⍺13, ⍺14 sums upto 1 <br>

Finally these weights are plugged in the original equation as defined above to obtain the context vector c1</br>

Eventually using the same we calculate the next hidden state s2 as </br>
s2 = DecoderRNN(s1, y1, c1)</br>
y2 = softmax(Linear(s2))
















